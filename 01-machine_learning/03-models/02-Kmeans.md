> 详细手推Kmeans 


### K-means聚类算法详细推导

K-means是一种非监督学习算法，用于将数据分成 $ k $ 个簇。它的目标是通过迭代优化来最小化簇内样本的平方误差和（Sum of Squared Errors, SSE）。

#### 算法步骤

1. **初始化**：
   - 随机选择 $ k $ 个簇中心（质心）。
   
2. **分配样本到最近的簇中心**：
   - 对于每个样本 $ x_i $，计算它到每个簇中心的距离，将其分配到距离最近的簇。

3. **更新簇中心**：
   - 计算每个簇的中心（均值），即所有属于该簇的样本的均值。

4. **重复步骤2和步骤3**，直到簇中心不再变化或变化很小。

#### 数学推导

假设我们有 $ n $ 个样本 $ \{x_1, x_2, \ldots, x_n\} $，每个样本都是 $ d $ 维向量，目标是将它们分成 $ k $ 个簇。

1. **初始化簇中心**：
   - 随机选择 $ k $ 个初始簇中心 $ \{\mu_1, \mu_2, \ldots, \mu_k\} $。

2. **样本分配**：
   - 对于每个样本 $ x_i $，计算它到每个簇中心的距离：
     $$
     d(x_i, \mu_j) = \|x_i - \mu_j\|^2
     $$
   - 将样本 $ x_i $ 分配到距离最近的簇 $ C_j $：
     $$
     C_j = \{ x_i : \|x_i - \mu_j\|^2 \leq \|x_i - \mu_l\|^2, \forall l = 1, 2, \ldots, k \}
     $$

3. **更新簇中心**：
   - 对于每个簇 $ C_j $，计算新的簇中心（均值）：
     $$
     \mu_j = \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i
     $$

4. **重复步骤2和步骤3**，直到簇中心不再变化或变化很小。

#### 目标函数

K-means的目标是最小化簇内样本的平方误差和（SSE）：
$$
J = \sum_{j=1}^{k} \sum_{x_i \in C_j} \|x_i - \mu_j\|^2
$$

通过迭代优化样本分配和簇中心更新，K-means算法试图找到一个局部最优解，使得SSE最小。

### 示例

假设我们有一组二维数据点：

$$
\{(1,2), (2,3), (3,4), (8,7), (9,8), (10,9)\}
$$

我们希望将它们分成 $ k = 2 $ 个簇。

1. **初始化**：
   - 随机选择两个初始簇中心，如 $ \mu_1 = (1,2) $ 和 $ \mu_2 = (8,7) $。

2. **样本分配**：
   - 计算每个样本到簇中心的距离，并进行分配。
   - 例如，点 (1,2) 到 $ \mu_1 $ 的距离为 0，到 $ \mu_2 $ 的距离为 $ \sqrt{(1-8)^2 + (2-7)^2} = \sqrt{49 + 25} = 8.6 $，因此 (1,2) 分配给簇 $ C_1 $。

3. **更新簇中心**：
   - 计算每个簇的新中心。
   - 例如，新的 $ \mu_1 $ 是 $ C_1 $ 中所有点的均值。

4. **重复**：
   - 继续分配样本到新的簇中心，并更新簇中心，直到簇中心不再变化。

### 参考资料

1. **Pattern Recognition and Machine Learning by Christopher M. Bishop**：
   - 提供了K-means算法的详细理论和推导。
   - [书籍链接](https://www.springer.com/gp/book/9780387310732)

2. **The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman**：
   - 介绍了K-means聚类及其他机器学习算法。
   - [书籍链接](https://web.stanford.edu/~hastie/ElemStatLearn/)