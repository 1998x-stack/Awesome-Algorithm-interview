> 详细手推LR 


### 逻辑回归（Logistic Regression）手推导

逻辑回归是一种用于二分类问题的线性模型，其目的是估计输入特征与输出标签之间的关系。逻辑回归通过使用Sigmoid函数将线性回归的输出映射到(0, 1)区间，从而用于概率估计。

#### 1. 模型假设

逻辑回归模型假设输出 $ y $ 与输入 $ \mathbf{x} $ 的关系如下：
$$ h(\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b) $$
其中，$ \sigma(z) $ 是Sigmoid函数，定义为：
$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

#### 2. 损失函数

逻辑回归使用对数似然损失函数。对于单个样本 $ (x_i, y_i) $，损失函数为：
$$ \ell(h(x_i), y_i) = -y_i \log(h(x_i)) - (1 - y_i) \log(1 - h(x_i)) $$

对于整个训练集，损失函数为：
$$ L(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(h(x_i)) + (1 - y_i) \log(1 - h(x_i)) \right] $$
其中，$ m $ 是样本数量。

#### 3. 梯度计算

为了最小化损失函数，我们需要计算损失函数相对于模型参数 $ \mathbf{w} $ 和 $ b $ 的梯度。

对于权重 $ \mathbf{w} $：
$$ \frac{\partial L}{\partial \mathbf{w}} = \frac{1}{m} \sum_{i=1}^{m} (h(x_i) - y_i) x_i $$

对于偏置 $ b $：
$$ \frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (h(x_i) - y_i) $$

#### 4. 梯度下降更新

使用梯度下降算法来更新模型参数。更新公式为：

$$ \mathbf{w} := \mathbf{w} - \alpha \frac{\partial L}{\partial \mathbf{w}} $$
$$ b := b - \alpha \frac{\partial L}{\partial b} $$

其中， $ \alpha $ 是学习率。

#### 5. 详细推导步骤

**步骤 1**：计算假设函数 $ h(\mathbf{x}) $ 的输出：

$$ h(x_i) = \sigma(\mathbf{w}^T x_i + b) = \frac{1}{1 + e^{-(\mathbf{w}^T x_i + b)}} $$

**步骤 2**：计算损失函数：

$$ L(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(h(x_i)) + (1 - y_i) \log(1 - h(x_i)) \right] $$

**步骤 3**：计算损失函数对权重 $ \mathbf{w} $ 的偏导数：

$$ \frac{\partial L}{\partial \mathbf{w}} = -\frac{1}{m} \sum_{i=1}^{m} \left[ \frac{y_i (1 - h(x_i)) x_i - (1 - y_i) h(x_i) x_i}{h(x_i) (1 - h(x_i))} \right] $$

利用 $ h(x_i) = \frac{1}{1 + e^{-(\mathbf{w}^T x_i + b)}} $ 和 $ 1 - h(x_i) = \frac{e^{-(\mathbf{w}^T x_i + b)}}{1 + e^{-(\mathbf{w}^T x_i + b)}} $，可以化简为：

$$ \frac{\partial L}{\partial \mathbf{w}} = \frac{1}{m} \sum_{i=1}^{m} (h(x_i) - y_i) x_i $$

**步骤 4**：计算损失函数对偏置 $ b $ 的偏导数：

$$ \frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (h(x_i) - y_i) $$

**步骤 5**：更新权重和偏置：

$$ \mathbf{w} := \mathbf{w} - \alpha \frac{\partial L}{\partial \mathbf{w}} $$
$$ b := b - \alpha \frac{\partial L}{\partial b} $$

### 参考文献

1. **Pattern Recognition and Machine Learning by Christopher M. Bishop**：
   - 提供了逻辑回归的详细理论和推导。
   - [书籍链接](https://www.springer.com/gp/book/9780387310732)

2. **The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman**：
   - 介绍了逻辑回归、梯度下降以及其他机器学习算法。
   - [书籍链接](https://web.stanford.edu/~hastie/ElemStatLearn/)